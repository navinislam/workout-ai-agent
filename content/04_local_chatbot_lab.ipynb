{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6feea88",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "# Introduction to Local LLMs with Ollama\n",
    "\n",
    "Time to get practical!\n",
    "\n",
    "This notebook demonstrates how to run a local Large Language Model (LLM) using Ollama and LangChain. This will run entirely on your computer. \n",
    "\n",
    "Ollama allows us to run LLMs locally on our machine. LangChain is an SDK (library i.e. re-usable code) which makes it easy to interact with LLMs. \n",
    "\n",
    "## 1. Prerequisites\n",
    "\n",
    "Before running this notebook, you need to install Ollama and download a model. Follow the steps below.\n",
    "\n",
    "### 1.1 Installing Ollama\n",
    "\n",
    "**macOS:**\n",
    "- Download the installer from [ollama.ai](https://ollama.ai)\n",
    "- Open the downloaded file and follow the installation prompts\n",
    "\n",
    "**Windows:**\n",
    "- Download the installer from [ollama.ai](https://ollama.ai)\n",
    "- Run the installer and follow the instructions\n",
    "\n",
    "**Linux:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ed4976",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -fsSL https://ollama.ai/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9a2cd7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Verify your installation (Windows, Mac, and Linux):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6da91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ollama --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeefb3e1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### 1.2 Downloading the TinyLlama Model\n",
    "\n",
    "Download the TinyLlama model (about 600MB). There is a big catalog of models you can download from Ollama including DeepSeek and others, but TinyLlama is the smallest, which is good for demo purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0178721",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ollama pull tinyllama:1.1b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd55eab0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## 2. Setting Up the Python Environment\n",
    "\n",
    "Install the required packages. We will install Langchain, which is one of the most popular libraries to interacat with LLMs. Langchain-ollama allows us to interact with models locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c100aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install langchain langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5feaaf",
   "metadata": {},
   "source": [
    "## 3. Using the LLM with LangChain\n",
    "\n",
    "The code below allows us to interact with the tinyllama model hosted locally on our computers. We are simply sending the following message to the model: \"I love programming\".\n",
    "\n",
    "Please read the code comments if you want to understand exactly what's going on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989512b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# Initialize the LLM. This is when we pick the model and the temperature (which controls the randomness of the output). llm is what we will use to call the model.\n",
    "llm = ChatOllama(\n",
    "    model=\"tinyllama:1.1b\",\n",
    "    temperature=0,  # 0 for more deterministic outputs\n",
    ")\n",
    "\n",
    "# We need to provide an array of messages to the model. The first message is always the system message, which tells the model what it is. The second message is the human message, which is what we want to ask the model.\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "\n",
    "# Send the array of messages to the model and get the response. The response is an AIMessage object, which contains the content of the message. Which we will print below.\n",
    "ai_msg = llm.invoke(messages)\n",
    "\n",
    "# Display the response\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ab5a6",
   "metadata": {},
   "source": [
    "## 4. Testing Different Prompts\n",
    "\n",
    "Let's try a couple of examples to see what the model can do by changing the messaging array and using a different message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aa4acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask for an explanation\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful and informative AI assistant.\",\n",
    "    ),\n",
    "    (\"human\", \"Explain the concept of a neural network in simple terms.\"),\n",
    "]\n",
    "\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf603f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask for some code\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are an expert Python programmer.\",\n",
    "    ),\n",
    "    (\"human\", \"Write a function to check if a string is a palindrome.\"),\n",
    "]\n",
    "\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed492d41",
   "metadata": {},
   "source": [
    "## 5. Creating a Basic Chatbot\n",
    "\n",
    "Let's create a simple chat interface so we can interact with the model. Each cell will represent an interaction. Follow the steps below, they will make more sense when you go through them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238d483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Run this cell first\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOllama(\n",
    "    model=\"tinyllama:1.1b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# System prompt\n",
    "system_prompt = \"You are a helpful assistant\"\n",
    "\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1c0b56",
   "metadata": {},
   "source": [
    "Now let's create a cell for our first interaction by telling the model what our favorite color is. Note that the response might be weird, again we're not using the best model out there. But it should indicate that it understood that your favorite color is blue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db32f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First interaction \n",
    "user_message = \"My favorite color is blue.\"\n",
    "\n",
    "# Create fresh messages for this interaction only\n",
    "messages = [\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", user_message)\n",
    "]\n",
    "\n",
    "# Get response\n",
    "ai_msg = llm.invoke(messages)\n",
    "\n",
    "print(f\"You: {user_message}\")\n",
    "print(f\"Assistant: {ai_msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc9f698",
   "metadata": {},
   "source": [
    "Now let's follow up with a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c242b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second interaction\n",
    "user_message = \"What's my favorite color?\"\n",
    "\n",
    "# Create fresh messages for this interaction only\n",
    "messages = [\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", user_message)\n",
    "]\n",
    "\n",
    "# Get response\n",
    "ai_msg = llm.invoke(messages)\n",
    "\n",
    "print(f\"You: {user_message}\")\n",
    "print(f\"Assistant: {ai_msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e27722d",
   "metadata": {},
   "source": [
    "As you can see, the model doesn't remember our favorite color! You might ask why? Surely tools like ChatGPT and others can remember the full context of the conversation. This is the concept of memory. The LLM, by itself, does not remember past conversations. We need to engineer our application so that it does.\n",
    "\n",
    "Let us explore how we can do it below, and how tools like ChatGPT manage that. \n",
    "\n",
    "\n",
    "## 6. Understanding How Conversation Memory Works\n",
    "\n",
    "The easiest way to manage that is by simply passing the entire conversation history to the model, instead of just the individual message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365adc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "# Initialize the LLM like before\n",
    "llm = ChatOllama(\n",
    "    model=\"tinyllama:1.1b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Define the conversation history (always starts with a system prompt)\n",
    "conversation = [SystemMessage(content=\"You are a helpful assistant\")] #conversation is an array (list) of messages. SystemMessage is a class that represents a system message and accepts content as a parameter.\n",
    "\n",
    "#Define a function called chat which takes user input as a parameter. This function will be used to interact with the model.\n",
    "def chat(user_input: str):\n",
    "    # 1) Append the user's message\n",
    "    conversation.append(HumanMessage(content=user_input)) #We add the user message to the conversation list. Now we have the system message and the user message in the conversation list.\n",
    "    \n",
    "    # 2) Call the model with the full conversation list (user message + system message)\n",
    "    ai_msg = llm(conversation)\n",
    "    \n",
    "    # 3) Display the exchange\n",
    "    print(f\"You: {user_input}\")\n",
    "    print(f\"Assistant: {ai_msg.content}\")\n",
    "    print(f\"[Conversation length: {len(conversation) + 1} messages]\")  # +1 for the AI response\n",
    "    \n",
    "    # 4) Add the AI's response to history. So now we have the system message, user message, and AI response in the conversation list which will be used for the next interaction.\n",
    "    conversation.append(ai_msg)\n",
    "    \n",
    "    return ai_msg.content\n",
    "\n",
    "print(\"Chat With Memory\")\n",
    "print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e3433e",
   "metadata": {},
   "source": [
    "Let's give our favorite color again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81d27fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First interaction - share favorite color\n",
    "chat(\"My favorite color is blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ac723",
   "metadata": {},
   "source": [
    "Now let's see if it remembers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526f140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second interaction - test memory\n",
    "chat(\"What's my favorite color?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2cc0ce",
   "metadata": {},
   "source": [
    "Now the model should correctly remember your name! This is because we're keeping track of the conversation history.\n",
    "\n",
    "## 7. The Context Window Challenge\n",
    "\n",
    "While our simple memory solution works initially, because we simply passed all message history as an input, it has a critical limitation: **the context window**.\n",
    "\n",
    "### Understanding the Context Window\n",
    "\n",
    "Every language model has a fixed \"context window\" - the maximum number of tokens (roughly words or word pieces) it can process at once:\n",
    "\n",
    "- **TinyLlama**: ~2,048 tokens (about 1,500 words)\n",
    "- **GPT-3.5**: ~4,096 tokens\n",
    "- **GPT-4**: ~8,192 to 128,000 tokens depending on the version\n",
    "- **Claude 3**: Up to 200,000 tokens\n",
    "\n",
    "As the conversation grows, we eventually hit this limit. When that happens:\n",
    "\n",
    "1. The model can't see messages beyond the window size\n",
    "2. It effectively \"forgets\" the earliest parts of the conversation \n",
    "3. Processing becomes slower with larger contexts\n",
    "4. You may receive errors if you exceed the context window\n",
    "\n",
    "## 8. Solutions to the Context Window Problem\n",
    "\n",
    "In production systems like ChatGPT, several techniques address the context window limitation:\n",
    "\n",
    "### 1. Sliding Context Window\n",
    "This approach keeps only the most recent N messages, plus the system prompt.\n",
    "\n",
    "**Example:** If you limit to 10 messages, and your conversation reaches 15 messages, you'd drop the 5 oldest messages (except the system prompt).\n",
    "\n",
    "This is like having a conversation where you remember only what was said in the last few minutes. It's simple but loses all older information.\n",
    "\n",
    "### 2. Summarization\n",
    "This technique condenses older parts of the conversation into summaries to save token space.\n",
    "\n",
    "**Example:** After 10 back-and-forth exchanges, the system might replace those 20 messages with a single summary: \"User introduced themselves as Alex and asked about neural networks. Assistant explained neural networks and provided a Python code example for palindrome detection.\"\n",
    "\n",
    "This preserves the key points while reducing token usage significantly.\n",
    "\n",
    "### 3. Database-Backed Session Management\n",
    "\n",
    "This approach uses a database to store complete conversation histories associated with unique user sessions. Here's how it works:\n",
    "\n",
    "1. **Session Creation**: When a user starts chatting, the system creates a unique session ID (like \"session_abc123\")\n",
    "\n",
    "2. **Message Storage**: Every message from the user and AI is stored in a database table linked to that session ID\n",
    "\n",
    "3. **Context Window Management**: When preparing the prompt for the LLM, the system:\n",
    "   - Retrieves all messages for the session\n",
    "   - Applies strategies to fit within the context window (like sliding window or summarization)\n",
    "   - Sends the optimized conversation to the model\n",
    "   - Stores the new response back in the database\n",
    "\n",
    "**Example in Practice:**\n",
    "- A user has chatted for hours with ChatGPT\n",
    "- Their session ID \"user_789\" now has 200 messages in the database\n",
    "- When they send message #201, the system:\n",
    "  - Retrieves all 200 previous messages from the database\n",
    "  - Selects the most important ones to fit in the context window\n",
    "  - Gets a response from the model\n",
    "  - Adds message #201 and the response to the database\n",
    "  - Even if the model only \"sees\" the recent portion, the full history remains in the database\n",
    "\n",
    "This is how services like ChatGPT can maintain \"memory\" across very long conversations and even when you close your browser and come back later.\n",
    "\n",
    "## What ChatGPT Typically Uses\n",
    "\n",
    "ChatGPT uses a hybrid approach that combines several of these techniques:\n",
    "\n",
    "1. **Primary Technique:** It uses a very large context window (up to 128K tokens in GPT-4o) and database-backed session management, allowing it to \"remember\" remarkably long conversations.\n",
    "\n",
    "2. **For Long Conversations:** When conversations exceed even these generous limits, it employs sliding window techniques that prioritize:\n",
    "   - The system prompt/instructions\n",
    "   - The most recent messages\n",
    "   - Messages with high information density\n",
    "   - Messages explicitly referenced by the user\n",
    "\n",
    "3. **Dynamic Compression:** In some versions, ChatGPT also employs dynamic compression algorithms that selectively summarize or remove parts of the conversation that appear less relevant to the current exchange.\n",
    "\n",
    "While these solutions help, there's still an absolute limit to what any model can \"remember\" in a single conversation. This is why even ChatGPT sometimes \"forgets\" things mentioned much earlier in very long conversations.\n",
    "\n",
    "## 9. Benefits of Local LLMs\n",
    "\n",
    "Using local LLMs with tools like Ollama has several advantages:\n",
    "\n",
    "1. **Privacy**: Your data never leaves your computer\n",
    "2. **No API costs**: Run the model as much as you want without paying per query\n",
    "3. **Offline usage**: No internet connection required once the model is downloaded\n",
    "4. **No rate limits**: Run as many queries as your hardware can handle\n",
    "\n",
    "## 10. Resources for Further Learning\n",
    "\n",
    "- [Ollama Documentation](https://github.com/ollama/ollama/blob/main/README.md)\n",
    "- [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction)\n",
    "- [TinyLlama Model Information](https://github.com/jzhang38/TinyLlama)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
