{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "942335d7",
   "metadata": {},
   "source": [
    "\n",
    "# Building Interactive AI Applications with Gradio\n",
    "\n",
    "This lab guides you through creating interactive AI applications using Gradio, focusing on building a customizable chatbot with streaming responses.\n",
    "\n",
    "Gradio is a python frontend library which makes it very easy to build chatbot experiences. \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, make sure you have the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e9dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install gradio openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580ae89a",
   "metadata": {},
   "source": [
    "## 1. Setting Up Your Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaabe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (if using .env file)\n",
    "load_dotenv()\n",
    "\n",
    "# IMPORTANT: Set your API key directly (for testing only)\n",
    "# NOTE: For security reasons, you should use a .env file instead in production\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "# Now initialize the OpenAI client\n",
    "\n",
    "# Verify API key\n",
    "if os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"OpenAI API Key loaded successfully\")\n",
    "else:\n",
    "    print(\"ERROR: OpenAI API Key not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d63286",
   "metadata": {},
   "source": [
    "## 2. Your First Gradio Interface\n",
    "\n",
    "Let's create a simple Gradio interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb51446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will define a simple function called echo, all it does is it will accept a message and simply return it back!\n",
    "def echo(message):\n",
    "    return f\"You said: {message}\"\n",
    "\n",
    "# Create a basic Gradio interface. gr.interface() is a function that creates a web-based UI for your function, basically what that means is that it will create a web page where you can enter text and see the output.\n",
    "# That interface will be hosted on a local server, and you can access it via your web browser.\n",
    "# The interface accepts the function to be called (echo in this case), the input type (textbox), and the output type (also textbox). The title and description are optional, but they help to provide context for the user. Flagging mode is set to \"never\" to disable the flagging feature to make it look better.\n",
    "# when you run this function, it will spin up an interface with input and output boxes, and a button to submit the input. When you submit the input, it will call the echo function with the input text and display the output in the output box.\n",
    "demo = gr.Interface(\n",
    "    fn=echo,\n",
    "    inputs=\"textbox\",\n",
    "    outputs=\"textbox\",\n",
    "    title=\"Echo Bot\",\n",
    "    description=\"A simple bot that echoes what you type\",\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a73821d",
   "metadata": {},
   "source": [
    "## 3. Building a Basic Chatbot\n",
    "\n",
    "Let's create a simple chatbot using OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427d3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get response from OpenAI. We already went through that before. \n",
    "def get_response(message):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# We will create the same exact gradio interface as before, but we will use get_response function instead of echo! The get_response function will call the Open AI with the message and return the response. The response will be displayed in the output box.\n",
    "# Basically, the user will enter a message in the input box, and when they click the submit button, it will call the get_response function with the input text and display the output in the output box.\n",
    "\n",
    "chatbot = gr.Interface(\n",
    "    fn=get_response,\n",
    "    inputs=gr.Textbox(placeholder=\"Ask me anything...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Simple AI Chatbot\",\n",
    "    description=\"Chat with an AI assistant powered by OpenAI\",\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "# Launch the chatbot\n",
    "chatbot.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68d20ad",
   "metadata": {},
   "source": [
    "## 4. Creating a Streaming Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to stream responses from OpenAI\n",
    "def stream_response(message):\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            response += chunk.choices[0].delta.content\n",
    "            yield response # yield is a python keyword that allows you to return a value from a function without terminating the function. This means that the function can be called again later, and it will continue from where it left off. This is useful for streaming responses, as it allows you to return partial results as they are generated.\n",
    "            # so this is instead of return, which would terminate the function.\n",
    "\n",
    "# Create a streaming chatbot interface\n",
    "streaming_chatbot = gr.Interface(\n",
    "    fn=stream_response,\n",
    "    inputs=\"textbox\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Streaming AI Chatbot\",\n",
    "    description=\"Chat with an AI assistant that streams responses in real-time\",\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "# Launch the streaming chatbot\n",
    "streaming_chatbot.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e1d00",
   "metadata": {},
   "source": [
    "## 5. Building a Chatbot with Memory\n",
    "\n",
    "Now let's create a more advanced chatbot that remembers conversation history. Again, we already went through that process before, we are just adding it to a Gradio interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9ae82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function called chat, which accepts the user message and conversation history\n",
    "def chat(message, history):\n",
    "    # Initialize messages with system prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "    \n",
    "    # We are looping through the history of messages (initially just an empty list). Entry is a single message in the history\n",
    "    for entry in history:\n",
    "        # \n",
    "        if isinstance(entry, list) and len(entry) == 2:\n",
    "            user_msg, assistant_msg = entry\n",
    "            # Add user message\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            # Add assistant message if it exists\n",
    "            if assistant_msg:\n",
    "                messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        # If history is already in OpenAI format with role and content\n",
    "        elif isinstance(entry, dict) and \"role\" in entry and \"content\" in entry:\n",
    "            messages.append(entry)\n",
    "    \n",
    "    # Add the new user message to the conversation\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    # Get streaming response from OpenAI\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    # Return the response chunk by chunk as it arrives\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            response += chunk.choices[0].delta.content\n",
    "            yield response\n",
    "\n",
    "# Create and launch the chatbot interface\n",
    "memory_chatbot = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    title=\"AI Chatbot with Memory\",\n",
    "    description=\"Chat with an AI assistant that remembers your conversation history.\",\n",
    "    examples=[\"Tell me about machine learning\", \"How do neural networks work?\"],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "memory_chatbot.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3a8177",
   "metadata": {},
   "source": [
    "## 6. Restaurant Menu Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4b457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a restaurant menu\n",
    "def generate_menu(restaurant_name, cuisine_type, special_requirements=\"None\"):\n",
    "    prompt = f\"\"\"\n",
    "    Create a restaurant menu for \"{restaurant_name}\", a {cuisine_type} restaurant.\n",
    "    Special requirements: {special_requirements}\n",
    "    \n",
    "    Include:\n",
    "    - 3 appetizers\n",
    "    - 4 main courses\n",
    "    - 2 desserts\n",
    "    \n",
    "    For each item, include a name, brief description, and price.\n",
    "    \n",
    "    Format your response in markdown with appropriate headers, styling, and sections.\n",
    "    Add a brief introduction about the restaurant at the top.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert restaurant consultant who creates beautiful, well-formatted menus.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Create a menu generator interface\n",
    "menu_generator = gr.Interface(\n",
    "    fn=generate_menu,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Restaurant Name\"),\n",
    "        gr.Textbox(label=\"Cuisine Type (e.g., Italian, Japanese)\"),\n",
    "        gr.Textbox(label=\"Special Requirements (Optional)\", placeholder=\"e.g., vegetarian options\")\n",
    "    ],\n",
    "    outputs=gr.Markdown(label=\"Generated Menu\"),\n",
    "    title=\"Restaurant Menu Generator\",\n",
    "    description=\"Create a professional restaurant menu with AI\",\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "# Launch the menu generator\n",
    "menu_generator.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036418f",
   "metadata": {},
   "source": [
    "## 7. Resources for Further Learning\n",
    "\n",
    "- [Gradio Documentation](https://www.gradio.app/docs/)\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
