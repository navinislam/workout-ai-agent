{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70ad079d",
   "metadata": {},
   "source": [
    "# Complete RAG System Implementation with Semantic Kernel\n",
    "\n",
    "This comprehensive notebook demonstrates how to build a Retrieval Augmented Generation (RAG) system using Microsoft's Semantic Kernel. We'll start by showing the limitations of AI models without access to specific data, then build a complete RAG system with chunking strategies, vector databases, and evaluation methods.\n",
    "\n",
    "## Installation and Setup\n",
    "\n",
    "In this module, we will leverage Semantic Kernel. \n",
    "\n",
    "Semantic Kernel is an open source SDK that allows you to easily build Ai applications. It supports C#, Python, and Java. It is production ready and many large enterprises leverage it. \n",
    "\n",
    "It is designed to be modular, so you can easily change models without rewriting your entire codebase. \n",
    "\n",
    "SDKs like Semantic Kernel became popular because LLMs themselves can only process data and generate responses. It can’t access your database, call your APIs, execute code, or interact with external systems. So Semantic Kernel manages connections to AI services (like Open AI), provides a plugin system where you can write functions that the AI can call, and manages conversation history and context.\n",
    "\n",
    "At the heart of semantic kernel, is the Kernel orchestrator. In AI applications, you need to coordinate multiple moving parts like AI services, databases, APIs, logging systems, etc. Kernel is the central orchestrator that holds all of these together. It contains Services (like AI Services, login services, authentication services) and Plugins (custom functions the AI can call, like accessing your database). Consider a real enterprise scenario, where an AI assistant that needs to query CRM, check inventory levels, generate proposal, and log all interactions for compliance. Without a kernel, every piece of code will need to know how to connect to all these services. With a kernel, everything is configured once. Because all AI operations flow through the kernel, you have a single point of control for logging and management. Kernel now supports MCP, which wraps the kernel in a network aware sever that speaks the MCP language so that this kernel (or agent) is discoverable by others. \n",
    "\n",
    "Semantic Kernel Components:\n",
    "\n",
    "1. AI Service Connectors: Of course we live in a world where we use multiple AI models, and these models use different APIs and authentication methods. A connector in SK is an abstraction layer to prevent vendor lock-in (allows you to change between multiple models). For instance, AzureChatCompletion is one Service, GoogleAIChatCompletion is another service. The kernel is responsible for calling these connectors.\n",
    "2. Vector Store Connectors: This is the core of RAG. This is the bridge between vectors stores and the kernel.\n",
    "3. Functions and Plugins: Plugins is what allows you to allow these LLMs to have access to tools. A function is a single capability you expose to the LLM (e.g. a python function) and a plugin is a group of related functions (DatabasePlugin might contain functions like GetUser, UpdateUser, GetOrders). \n",
    "4. Prompt Templates: Writing effective prompts with multi-line strings inside our code can get messy and hard to maintain. It is also impossible for non-developers like a prompt engineer to work with. Practically, it is either a text file or a string that mixes static instructions with dynamic placeholder. The static instruction can be: “You are an expert financial analyst, summarize the following report” and the dynamic placeholders could be user_input, or a function to get stock price. \n",
    "5. Filters: Piece of code to intercept the kernel execution at key moments, like before a function is called or after a prompt is rendered. You can do this to filter PII data (ensuring no user credit card number is ever sent to the external LLM).\n",
    "\n",
    "\n",
    "**First, install the required packages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9ed35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell first to install all required packages\n",
    "!pip install semantic-kernel openai numpy scikit-learn faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4c7196",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d6dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Semantic Kernel core imports (verified working June 2025)\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, OpenAITextEmbedding\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatPromptExecutionSettings\n",
    "\n",
    "\n",
    "# For vector storage - we'll build our own simple system\n",
    "import faiss\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"Semantic Kernel environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b43f176",
   "metadata": {},
   "source": [
    "## Creating Our Document Model and Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca15f662",
   "metadata": {},
   "source": [
    "We will use FAISS vector database. FAISS is a local vector database library that runs entirely on your machine. In production scenarios, we typically use a cloud vector database like Azure Search. But the concepts are the same, a vector database allows us to store vectors in it. \n",
    "\n",
    "Code explanation: \n",
    "\n",
    "1. We will create a class (to create an object out of it i.e. instance of class) to represent document chunks. There will be detailed comments to capture what will be inside these chunk objects. \n",
    "2. Then we will create another class that contains functions (methods) which allow us to search for these chunks using vector search (instead of exact word matches). \n",
    "    - Initialization function: A function to initialize our vector store. We will initiaize our vector store with these properties:\n",
    "        - Embedding Dimension: Embedding Dimension: How many numbers each document chunk will have when converter to vectors. Open AI’s text embedding model (which we will use) converts any text into exactly 1536 numbers. So “Hello world” becomes [0.123, -0.456, 0.789, … 1536 numbers in total). Every piece of text gets exactly 1536 numbers, whether it’s one word or a paragraph. \n",
    "        - Index: Think of an index like table of contents, instead of searching every row in a database to find “John Smith”, the index tells you which row John is in. In vector databases, an index is a data structure that organizes vectors so you can quickly find similar ones. Without an index, finding similar vectors would require comparing your query against every single stored vector (one by one). With an index, FAISS pre-organizes the vectors so ti can quickly jump to the most similar ones. We are initializing that index. \n",
    "        - Documents: Stores the actual document chunks (original text plus metadata) in a regular Python list.\n",
    "        - ID to Index: This is a dictionary that maps document IDs to their position in our list. So we can quickly find document chunk 5 as an example without searching through the entire list. \n",
    "    - Add documents function: This takes a batch of document chunks and stores them in our vector store. We process each document (that is already vectorized via our embedding model) and then add them to the FAISS index for fast similarity search. We keep the original text and metadata in our documents list so we can retrieve it later. \n",
    "    - Search function: This takes a user’s question (that is converted to a 1536 number vector) and finds the most similar document chunks. \n",
    "3. We now have our custom built vector store. Now we need to connect an LLM to it and this is where Semantic Kernel comes in. We will use Semantic Kernel to tap into embedding capabilities (converting text into vectors) and chat completion capabilities (the LLM reasoning engine). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b1f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "# We will create a DocumentChunk class to represent each piece of a document\n",
    "class DocumentChunk:\n",
    "\n",
    "   # Required fields - every chunk must have these\n",
    "   id: str                    # Unique name for this chunk, like \"policy_doc_chunk_1\"\n",
    "   content: str              # The actual text content of this chunk \n",
    "   source_doc_id: str        # Which original document this came from\n",
    "   title: str                # Human-readable title of the original document\n",
    "   chunk_index: int          # Which piece is this? (0=first chunk, 1=second, etc.)\n",
    "   \n",
    "   # Optional fields - these have default values\n",
    "   department: str = \"\"      # Which team owns this document (optional)\n",
    "   doc_type: str = \"\"        # What kind of doc is this - policy, guide, etc. (optional)\n",
    "   embedding: List[float] = None  # The vector representation (list of numbers) for this text\n",
    "\n",
    "# This is a class with methods to search for documents. Instead of exact word matches, it finds documents with similar meanings. \n",
    "class SimpleVectorStore:\n",
    "  \n",
    "   \n",
    "   # This is a function to initialize our vector store. We will use FAISS, where we can store and search through many document chunks quickly. \n",
    "   def __init__(self, embedding_dimension: int = 1536):\n",
    "       # How many numbers are in each embedding vector? What this means is that each document chunk will be represented by a list of 1536 numbers capturing its meaning.\n",
    "       # OpenAI's text-embedding-ada-002 model (if we use it) gives us 1536 numbers for each piece of text.\n",
    "       self.embedding_dimension = embedding_dimension\n",
    "       \n",
    "       # Create a FAISS index to store our document embeddings. Index FlatIP means we will use inner product similarity (like cosine similarity) to find similar documents.\n",
    "       self.index = faiss.IndexFlatIP(embedding_dimension)\n",
    "       \n",
    "       # Store the actual document chunks (the text and metadata) in a list called documents\n",
    "       self.documents: List[DocumentChunk] = []\n",
    "       \n",
    "       # This dictionary maps document IDs to their position in the documents list, so we can quickly find a document by its ID. for example, if we have a document with ID \"policy_doc_chunk_1\", we can find it in our list of documents by looking up \"policy_doc_chunk_1\" in this dictionary.\n",
    "       # This is like a quick lookup table - if we have a document with ID \"policy_doc_chunk_1\", we can find it in our list of documents by looking up \"policy_doc_chunk_1\" in this dictionary.\n",
    "       self.id_to_index = {}\n",
    "   \n",
    "   #This function adds a batch of documents to our vector store.\n",
    "   def add_documents(self, documents: List[DocumentChunk]):\n",
    "      \n",
    "       embeddings = []  # This is where we will store the vector representations (embeddings) of each document chunk\n",
    "       \n",
    "       # Process each document one by one\n",
    "       for doc in documents:\n",
    "           # Every document must have its embedding (vector representation) already calculated\n",
    "           if doc.embedding is None:\n",
    "               raise ValueError(f\"Document {doc.id} missing embedding\")\n",
    "           \n",
    "           # CRITICAL: Normalize the embedding vector\n",
    "           # Why? So we can use cosine similarity (comparing angles, not lengths)\n",
    "           # Think of vectors as arrows - we want to compare which direction they point\n",
    "           embedding_array = np.array(doc.embedding)  # Convert list to numpy array for math\n",
    "           normalized_embedding = embedding_array / np.linalg.norm(embedding_array)  # Make length = 1\n",
    "           embeddings.append(normalized_embedding) # This is the normalized vector representation of the document chunk stored in our embeddings list\n",
    "           \n",
    "           # These are the metadata fields we will use to identify and retrieve the document later\n",
    "           doc_index = len(self.documents)           # What position will this doc be at?\n",
    "           self.documents.append(doc)                # Add document to our storage\n",
    "           self.id_to_index[doc.id] = doc_index     # Remember: this ID is at this position\n",
    "       \n",
    "       # Add all the normalized vectors to FAISS for lightning-fast search\n",
    "       embeddings_array = np.array(embeddings).astype('float32')  # FAISS requires float32 type\n",
    "       self.index.add(embeddings_array)\n",
    "       \n",
    "       print(f\"Added {len(documents)} documents to vector store\")\n",
    "   \n",
    "   # This function searches for documents similar to a user's question. \n",
    "   def search(self, query_embedding: List[float], k: int = 3, score_threshold: float = 0.0):\n",
    "       \"\"\"\n",
    "       Find documents most similar to a query\n",
    "       \n",
    "       How it works:\n",
    "       1. Take the query's embedding (vector representation)\n",
    "       2. Compare it to all stored document embeddings\n",
    "       3. Return the k most similar ones above the threshold\n",
    "       \n",
    "       Args:\n",
    "           query_embedding: The vector representation of user's question\n",
    "           k: How many results to return (top 3, top 5, etc.)\n",
    "           score_threshold: Only return docs with similarity above this score\n",
    "       \n",
    "       Returns:\n",
    "           List of (document, similarity_score) pairs, sorted by similarity\n",
    "       \"\"\"\n",
    "       # Edge case: if no documents stored, return empty\n",
    "       if self.index.ntotal == 0:\n",
    "           return []\n",
    "       \n",
    "       # Normalize the query embedding just like we did for stored documents\n",
    "       # This ensures fair comparison - we're comparing directions, not magnitudes\n",
    "       query_array = np.array(query_embedding)\n",
    "       normalized_query = query_array / np.linalg.norm(query_array)\n",
    "       \n",
    "       # Ask FAISS to find the k most similar vectors\n",
    "       # reshape(1, -1) because FAISS expects 2D array (rows=queries, cols=dimensions)\n",
    "       # Even though we only have 1 query, we need to format it as [[1, 2, 3, ...]]\n",
    "       scores, indices = self.index.search(normalized_query.reshape(1, -1).astype('float32'), k)\n",
    "       \n",
    "       # Convert FAISS results into our format\n",
    "       results = []\n",
    "       for score, idx in zip(scores[0], indices[0]):  # [0] because we only sent 1 query\n",
    "           # FAISS returns -1 if it runs out of documents before reaching k\n",
    "           # Also filter by minimum similarity score\n",
    "           if idx >= 0 and score >= score_threshold:\n",
    "               # Use the index to get the actual document from our storage\n",
    "               document = self.documents[idx]\n",
    "               results.append((document, float(score)))\n",
    "       \n",
    "       return results\n",
    "\n",
    "# Set up the AI services we'll use\n",
    "kernel = Kernel()  # Semantic Kernel is our AI orchestration framework\n",
    "\n",
    "# Service 1: Chat completion (generates responses to questions)\n",
    "chat_service = OpenAIChatCompletion(\n",
    "   ai_model_id=\"gpt-3.5-turbo\"  # Which OpenAI model to use for chat\n",
    ")\n",
    "kernel.add_service(chat_service)  # Register this service with the kernel. This allows us to use the chat service in our semantic kernel for generating responses to user queries.\n",
    "\n",
    "# Service 2: Text embedding (converts text into vector representations)\n",
    "embedding_service = OpenAITextEmbedding(\n",
    "   ai_model_id=\"text-embedding-ada-002\"  # OpenAI's embedding model\n",
    ")\n",
    "kernel.add_service(embedding_service)  # Register this service with the kernel\n",
    "\n",
    "#Now our kernel has both chat and embedding services ready to use. So we can ask questions and get answers, as well as convert text into vectors for similarity search.\n",
    "# We also set up a simple vector store using FAISS to store and search through document chunks based on their meanings.\n",
    "\n",
    "print(\"Semantic Kernel initialized with OpenAI services\")\n",
    "print(\"Using simple vector store with FAISS for document storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8700b9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Demonstrating the Problem - No Access to Private Data\n",
    "\n",
    "Let's start by showing what happens when we ask an AI model about information it wasn't trained on.\n",
    "\n",
    "The code below is simple, we have a series of \"documents\" stored in an array. We will ask questions about each of these documents (but we won't actually implement a proper RAG system) so we should expect the model not to know what we're talking about.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c18a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample company data that the model wouldn't know about.\n",
    "# This represents the private, \"ground-truth\" information.\n",
    "company_documents = [\n",
    "    {\n",
    "        \"id\": \"product_001\",\n",
    "        \"title\": \"CloudSync Pro Enterprise Plan\",\n",
    "        \"content\": \"\"\"CloudSync Pro Enterprise offers unlimited storage, advanced encryption, \n",
    "        real-time collaboration for up to 500 users, priority support, and custom integrations. \n",
    "        Pricing: $49/month per user with annual commitment. Features include: automatic backup, \n",
    "        version control, audit logs, SSO integration, and 99.9% uptime SLA.\"\"\",\n",
    "        \"metadata\": {\"department\": \"product\", \"type\": \"pricing\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"policy_001\", \n",
    "        \"title\": \"Remote Work Policy 2024\",\n",
    "        \"content\": \"\"\"Effective January 2024: All employees may work remotely up to 3 days per week. \n",
    "        Remote work requires approval from direct manager. Equipment stipend of $500 annually \n",
    "        for home office setup. Mandatory video calls for team meetings. Core hours: 10 AM - 3 PM \n",
    "        local time for collaboration.\"\"\",\n",
    "        \"metadata\": {\"department\": \"hr\", \"type\": \"policy\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"process_001\",\n",
    "        \"title\": \"Customer Refund Process\",\n",
    "        \"content\": \"\"\"Step 1: Customer submits refund request through support portal. \n",
    "        Step 2: Support agent reviews within 24 hours. Step 3: For amounts under $100, \n",
    "        automatic approval. Step 4: For amounts over $100, requires manager approval. \n",
    "        Step 5: Refunds processed within 3-5 business days to original payment method. \n",
    "        Full refunds available within 30 days of purchase.\"\"\",\n",
    "        \"metadata\": {\"department\": \"support\", \"type\": \"process\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"guide_001\",\n",
    "        \"title\": \"New Employee Onboarding Checklist\",\n",
    "        \"content\": \"\"\"Day 1: IT setup and system access. Day 2: Department orientation and mentor assignment. \n",
    "        Week 1: Complete mandatory training modules (security, compliance, company culture). \n",
    "        Week 2: Shadow team members and review project documentation. Month 1: Complete \n",
    "        probationary review and set 90-day goals.\"\"\",\n",
    "        \"metadata\": {\"department\": \"hr\", \"type\": \"guide\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "# The questions we want to test against the model's base knowledge.\n",
    "test_questions = [\n",
    "    \"What is the pricing for CloudSync Pro Enterprise?\",\n",
    "    \"How many days per week can employees work remotely?\",\n",
    "    \"What is the refund approval process for purchases over $100?\",\n",
    "    \"What happens during the first week of employee onboarding?\"\n",
    "]\n",
    "\n",
    "\n",
    "async def run_direct_to_model_test():\n",
    "    \"\"\"\n",
    "    Tests questions directly against the base AI model to demonstrate\n",
    "    its lack of knowledge about our private company data.\n",
    "    \"\"\"\n",
    "    print(\"TESTING MODEL WITHOUT RAG - Questions about private company data:\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    chat_service = kernel.get_service(type=OpenAIChatCompletion)\n",
    "\n",
    "    # ***FIX 1: Create a default settings object for OpenAI chat models.***\n",
    "    execution_settings = OpenAIChatPromptExecutionSettings()\n",
    "\n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\nQuestion {i}: {question}\")\n",
    "        \n",
    "        chat_history = ChatHistory()\n",
    "        chat_history.add_user_message(question)\n",
    "\n",
    "        # ***FIX 2: Pass the 'settings' object into the function call.***\n",
    "        response = await chat_service.get_chat_message_content(\n",
    "            chat_history=chat_history,\n",
    "            settings=execution_settings  # This argument is now required\n",
    "        )\n",
    "        \n",
    "        print(f\"Model Response: {str(response)}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# NOTE: This code assumes your Kernel is initialized and the OpenAI API key \n",
    "# is configured in your environment before running.\n",
    "\n",
    "print(\"✅ Starting test...\")\n",
    "# Run the single, simplified test function.\n",
    "await run_direct_to_model_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50323873",
   "metadata": {},
   "source": [
    "## What We Just Observed\n",
    "\n",
    "The model either:\n",
    "1. **Cannot answer** because it doesn't have access to this specific company information\n",
    "2. **Provides generic responses** that might not match your actual policies\n",
    "3. **Makes assumptions** that could be incorrect for your specific context\n",
    "\n",
    "This is exactly why we need RAG - to give the model access to your specific data while preserving its reasoning capabilities.\n",
    "\n",
    "---\n",
    "\n",
    "# Part 2: Document Chunking Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa4f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this section, we will explore different text chunking strategies. \n",
    "# We can either do a simple character-based split or a more semantic split that respects paragraphs and sentences. For example, on the latter, we will try to keep sentences together and avoid breaking them in the middle.\n",
    "\n",
    "# At a high level, what this function does is take a long piece of text and break it into smaller pieces (chunks) that are easier to work with.\n",
    "def simple_text_splitter(text: str, chunk_size: int = 300, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Simple character-based text splitter with overlap\n",
    "    \"\"\"\n",
    "    chunks = [] # This will hold our text chunks\n",
    "    start = 0 # Starting position in the text\n",
    "    \n",
    "    #Loop until we reach the end of the text.\n",
    "    while start < len(text):\n",
    "        # Calculate where this chunk should end\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        \n",
    "        # Try to end at a sentence boundary (but only if we're not at the very end)\n",
    "        if end < len(text):\n",
    "            last_period = text.rfind('.', start, end)\n",
    "            if last_period > start + chunk_size // 2:\n",
    "                end = last_period + 1\n",
    "        \n",
    "        # Extract the chunk\n",
    "        chunk = text[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        # Move to next position\n",
    "        # FIXED: Ensure we always move forward, even with large overlap\n",
    "        next_start = end - overlap\n",
    "        start = max(next_start, start + 1)  # Always move at least 1 character forward\n",
    "        \n",
    "        # If we've reached the end, break\n",
    "        if end >= len(text):\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# What this function does is take a long piece of text and break it into smaller pieces (chunks) that respect paragraph and sentence boundaries. So instead of just cutting it off at a certain number of characters, it tries to keep whole sentences together and avoid breaking them in the middle. This is useful because it helps preserve the meaning of the text and makes it easier to understand.\n",
    "def semantic_text_splitter(text: str, max_chunk_size: int = 400) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text respecting paragraph and sentence boundaries\n",
    "    \"\"\"\n",
    "    # Split by paragraphs first (handle both \\n\\n and single \\n)\n",
    "    paragraphs = [p.strip() for p in text.split('\\n') if p.strip()]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        # If this paragraph alone is too big, split it by sentences\n",
    "        if len(paragraph) > max_chunk_size:\n",
    "            sentences = [s.strip() for s in paragraph.split('.') if s.strip()]\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                sentence_with_period = sentence + '.' if not sentence.endswith('.') else sentence\n",
    "                \n",
    "                # Check if adding this sentence would exceed our limit\n",
    "                if current_chunk and len(current_chunk) + len(sentence_with_period) + 1 > max_chunk_size:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                    current_chunk = sentence_with_period\n",
    "                else:\n",
    "                    current_chunk += \" \" + sentence_with_period if current_chunk else sentence_with_period\n",
    "        else:\n",
    "            # Try to add the whole paragraph\n",
    "            if current_chunk and len(current_chunk) + len(paragraph) + 2 > max_chunk_size:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = paragraph\n",
    "            else:\n",
    "                current_chunk += \"\\n\" + paragraph if current_chunk else paragraph\n",
    "    \n",
    "    # Don't forget the last chunk\n",
    "    if current_chunk.strip():\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test different chunking strategies\n",
    "sample_doc = company_documents[0]\n",
    "print(\"CHUNKING STRATEGY COMPARISON:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "print(f\"Original document: {sample_doc['title']}\")\n",
    "print(f\"Length: {len(sample_doc['content'])} characters\")\n",
    "\n",
    "# Test simple chunking with safer parameters\n",
    "print(\"\\n1. SIMPLE CHARACTER-BASED CHUNKING:\")\n",
    "simple_chunks = simple_text_splitter(sample_doc['content'], chunk_size=200, overlap=30)  # Reduced overlap\n",
    "for i, chunk in enumerate(simple_chunks):\n",
    "    print(f\"Chunk {i+1} ({len(chunk)} chars): {chunk}\")\n",
    "\n",
    "# Test semantic chunking\n",
    "print(\"\\n2. SEMANTIC CHUNKING (respects paragraphs):\")\n",
    "semantic_chunks = semantic_text_splitter(sample_doc['content'], max_chunk_size=250)\n",
    "for i, chunk in enumerate(semantic_chunks):\n",
    "    print(f\"Chunk {i+1} ({len(chunk)} chars): {chunk}\")\n",
    "\n",
    "print(\"\\nTRADE-OFFS:\")\n",
    "print(\"- Simple chunking: Predictable sizes, may break mid-sentence\")\n",
    "print(\"- Semantic chunking: Preserves meaning, variable sizes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1b2f8b",
   "metadata": {},
   "source": [
    "## Testing the Complete RAG Pipeline\n",
    "\n",
    "Now we will implement a simple RAG system that uses the Semantic Kernel to handle document retrieval and answer generation.\n",
    "\n",
    "1. semantic_chunker: It’s job is to take a string of text and split it into smaller strings (chunks). It first splits text by paragraphs whenever it sees a double newline (\\n\\n) - this ensures that sentences that belong together stay together. Then it iterates through these paragraphs to group them into a chunk as per the max chunk size. By respecting natural breaks in the text, it ensures that related sentences stay together, creating high-quality, focused chunks of information. This dramatically increases the \"signal-to-noise\" ratio of our data.\n",
    "2. ingest_documents_semantic: This function is to solve the first major problem of any RAG system. Preparing data for AI. It accepts a list of documents, a vector)store (the custom built database we built earlier), and the embedding_service. It takes these documents and transforms them into vectors. It loops through each document, calls the embedding_service and creates a DocumentChunk object (with the vector, original text, and metadata).\n",
    "3. ask_with_semantic_rag: Core engine of RAG. It accepts the user’s question, the kernel, and the vector_store (our knowledge base). It passes the user question via an embedding service to get vector representation, then uses vector search method to find the document chunks whose vectors are closest. Then we augment the prompt and finally generate an answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b9069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Function for Semantic Chunking ---\n",
    "\n",
    "def semantic_chunker(text: str, max_chunk_size: int = 300) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits text into chunks, respecting paragraph boundaries to keep related sentences together.\n",
    "    \n",
    "    This is a pure utility function; it doesn't need any AI services or state.\n",
    "    Its only job is to intelligently split text based on structure.\n",
    "    \"\"\"\n",
    "    # First, split the text into paragraphs based on double newlines.\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    # Iterate through each paragraph to build chunks up to the max size.\n",
    "    for paragraph in paragraphs:\n",
    "        # If adding the next paragraph would make the current chunk too large...\n",
    "        if current_chunk and (len(current_chunk) + len(paragraph) + 2) > max_chunk_size:\n",
    "            # ...finalize the current chunk...\n",
    "            chunks.append(current_chunk)\n",
    "            # ...and start a new chunk with the current paragraph.\n",
    "            current_chunk = paragraph\n",
    "        else:\n",
    "            # Otherwise, add the paragraph to the current chunk.\n",
    "            current_chunk += (\"\\n\\n\" + paragraph) if current_chunk else paragraph\n",
    "            \n",
    "    # Add the last remaining chunk to the list.\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "# --- Core RAG Functions ---\n",
    "\n",
    "async def ingest_documents_semantic(\n",
    "    documents: List[Dict], \n",
    "    vector_store: SimpleVectorStore, \n",
    "    embedding_service: OpenAITextEmbedding\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Processes and ingests documents using the semantic chunking strategy.\n",
    "    \"\"\"\n",
    "    print(f\"Ingesting {len(documents)} documents with semantic chunking...\")\n",
    "    all_chunks_to_add = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Use our standalone helper function to get semantically coherent chunks.\n",
    "        text_chunks = semantic_chunker(doc[\"content\"])\n",
    "        \n",
    "        for i, chunk_text in enumerate(text_chunks):\n",
    "            # Skip chunks that are too short to have meaningful content.\n",
    "            if len(chunk_text) < 20:\n",
    "                continue\n",
    "\n",
    "            # Generate the embedding vector for the chunk's content.\n",
    "            embedding = (await embedding_service.generate_embeddings([chunk_text]))[0]\n",
    "            \n",
    "            # Create the DocumentChunk object.\n",
    "            chunk = DocumentChunk(\n",
    "                id=f\"{doc['id']}_chunk_{i}\",\n",
    "                content=chunk_text,\n",
    "                source_doc_id=doc[\"id\"],\n",
    "                title=doc[\"title\"],\n",
    "                chunk_index=i,\n",
    "                embedding=embedding\n",
    "            )\n",
    "            all_chunks_to_add.append(chunk)\n",
    "\n",
    "    vector_store.add_documents(all_chunks_to_add)\n",
    "    print(f\"Added {len(all_chunks_to_add)} new chunks to the vector store.\")\n",
    "\n",
    "\n",
    "async def ask_with_semantic_rag(\n",
    "    question: str, \n",
    "    kernel: Kernel, \n",
    "    vector_store: SimpleVectorStore\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Asks a question using the RAG pattern with the semantically chunked documents.\n",
    "    \"\"\"\n",
    "    # Get the necessary AI services from the kernel.\n",
    "    embedding_service = kernel.get_service(type=OpenAITextEmbedding)\n",
    "    chat_service = kernel.get_service(type=OpenAIChatCompletion)\n",
    "    \n",
    "    # 1. RETRIEVE: Convert the question to an embedding and search the vector store.\n",
    "    query_embedding = (await embedding_service.generate_embeddings([question]))[0]\n",
    "    search_results = vector_store.search(query_embedding, k=3, score_threshold=0.3)\n",
    "    \n",
    "    if not search_results:\n",
    "        return \"I could not find any relevant information in the documents to answer that question.\"\n",
    "        \n",
    "    # 2. AUGMENT: Build the context string from the retrieved document chunks.\n",
    "    context = \"\\n\\n---\\n\\n\".join([result.content for result, score in search_results])\n",
    "    \n",
    "    # Create the final prompt that instructs the AI and provides the context.\n",
    "    prompt = f\"\"\"\n",
    "Answer the following question based ONLY on the context provided below.\n",
    "\n",
    "CONTEXT:\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "    # 3. GENERATE: Send the augmented prompt to the chat model to get the final answer.\n",
    "    chat_history = ChatHistory()\n",
    "    chat_history.add_user_message(prompt)\n",
    "    \n",
    "    # Define execution settings for the AI call.\n",
    "    settings = OpenAIChatPromptExecutionSettings(max_tokens=200, temperature=0.1)\n",
    "    \n",
    "    response = await chat_service.get_chat_message_content(chat_history, settings)\n",
    "    \n",
    "    return str(response)\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "\n",
    "# 1. Initialize our vector store for this RAG process.\n",
    "semantic_vector_store = SimpleVectorStore()\n",
    "\n",
    "# 2. Get the embedding service from the kernel, as it's needed for ingestion.\n",
    "embedding_service = kernel.get_service(type=OpenAITextEmbedding)\n",
    "\n",
    "# 3. Call the ingestion function to process documents and populate the vector store.\n",
    "await ingest_documents_semantic(company_documents, semantic_vector_store, embedding_service)\n",
    "\n",
    "# 4. Ask a question using the populated vector store.\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TESTING RAG SYSTEM WITH SEMANTIC CHUNKING:\")\n",
    "question_to_ask = \"What is the pricing for CloudSync Pro Enterprise?\"\n",
    "answer = await ask_with_semantic_rag(question_to_ask, kernel, semantic_vector_store)\n",
    "\n",
    "print(f\"\\nQ: {question_to_ask}\")\n",
    "print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cad6f7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Advanced Configuration and Tuning\n",
    "\n",
    "We're testing two simple ways to make our RAG system give better answers - first, by changing how we ask the AI to respond (friendly vs professional style), and second, by adjusting how picky we are about which documents to include (strict matching vs loose matching).\n",
    "\n",
    "The latter is known as similarity threshold. A similarity threshold is like setting the bar for \"relevant\" search results. It's a number between 0 and 1 that determines how similar a document chunk must be to your question before we include it in the answer.\n",
    "\n",
    "When thresholds are too low: If you ask \"What is our vacation policy?\" with a threshold of 0.2, you might get results about vacation policy, employee benefits, time tracking, and company holidays. While all HR-related, this floods the user with information that's not directly answering their question. The AI then has to sift through all this extra context, potentially diluting the quality of the final answer.\n",
    "\n",
    "When thresholds are too high: If you ask \"How do I request time off?\" with a threshold of 0.7, you might get no results at all because no document contains that exact phrase, even though your vacation policy document clearly explains the process. Users end up frustrated with \"no information found\" responses when the answer actually exists in your knowledge base.\n",
    "\n",
    "Finding the sweet spot: The goal is to find a threshold that gives you enough relevant information without including noise. For most business documents, thresholds between 0.3-0.5 work well - high enough to filter out unrelated content, but low enough to catch relevant information that might use different wording than your question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f517be32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedRAG(SimpleRAG):\n",
    "    \"\"\"Simple RAG with optimization features\"\"\"\n",
    "    \n",
    "    async def ask_with_custom_prompt(self, question, prompt_template):\n",
    "        \"\"\"Ask question with a custom prompt\"\"\"\n",
    "        # Search for relevant chunks\n",
    "        query_embedding = await self.embedding_service.generate_embeddings([question])\n",
    "        results = self.vector_store.search(query_embedding[0], k=3, score_threshold=0.3)\n",
    "        \n",
    "        if not results:\n",
    "            return \"No relevant information found.\"\n",
    "        \n",
    "        # Build context\n",
    "        context = \"\\n\".join([doc.content for doc, score in results])\n",
    "        \n",
    "        # Use custom prompt template\n",
    "        prompt = prompt_template.format(context=context, question=question)\n",
    "        \n",
    "        # Generate answer\n",
    "        from semantic_kernel.connectors.ai.open_ai import OpenAIChatPromptExecutionSettings\n",
    "        \n",
    "        chat_history = ChatHistory()\n",
    "        chat_history.add_user_message(prompt)\n",
    "        settings = OpenAIChatPromptExecutionSettings(max_tokens=200, temperature=0.1)\n",
    "        \n",
    "        response = await self.chat_service.get_chat_message_content(chat_history, settings)\n",
    "        return str(response)\n",
    "    \n",
    "    async def test_thresholds(self, query, thresholds=[0.1, 0.3, 0.5, 0.7]):\n",
    "        \"\"\"Test different similarity thresholds\"\"\"\n",
    "        query_embedding = await self.embedding_service.generate_embeddings([query])\n",
    "        \n",
    "        print(f\"Testing thresholds for: '{query}'\")\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            results = self.vector_store.search(query_embedding[0], k=5, score_threshold=threshold)\n",
    "            \n",
    "            print(f\"\\nThreshold {threshold}: {len(results)} results\")\n",
    "            if results:\n",
    "                scores = [score for _, score in results]\n",
    "                print(f\"  Score range: {min(scores):.3f} - {max(scores):.3f}\")\n",
    "                print(f\"  Documents: {', '.join([doc.title for doc, _ in results[:2]])}\")\n",
    "\n",
    "# Create optimized RAG system\n",
    "opt_rag = OptimizedRAG(kernel)\n",
    "await opt_rag.add_documents(company_documents)\n",
    "\n",
    "# Test custom prompts\n",
    "customer_prompt = \"\"\"You are a helpful customer service agent. \n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Customer question: {question}\n",
    "\n",
    "Friendly response:\"\"\"\n",
    "\n",
    "employee_prompt = \"\"\"You are an internal HR assistant.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Employee question: {question}\n",
    "\n",
    "Professional response:\"\"\"\n",
    "\n",
    "# Test different prompt styles\n",
    "question = \"What is our remote work policy?\"\n",
    "\n",
    "print(\"CUSTOMER SERVICE STYLE:\")\n",
    "customer_answer = await opt_rag.ask_with_custom_prompt(question, customer_prompt)\n",
    "print(customer_answer)\n",
    "\n",
    "print(\"\\nHR ASSISTANT STYLE:\")\n",
    "hr_answer = await opt_rag.ask_with_custom_prompt(question, employee_prompt)\n",
    "print(hr_answer)\n",
    "\n",
    "# Test similarity thresholds\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "await opt_rag.test_thresholds(\"employee remote work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82976191",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Best Practices Summary\n",
    "\n",
    "### Document Processing\n",
    "- **Use semantic chunking** that respects paragraph and sentence boundaries\n",
    "- **Optimal chunk size: 300-400 characters** for most business documents\n",
    "- **Include meaningful overlap** (50-80 characters) to preserve context\n",
    "- **Preserve rich metadata** for filtering and source attribution\n",
    "\n",
    "### Vector Search Configuration\n",
    "- **Start with FAISS** for local development and small-scale production\n",
    "- **Use similarity thresholds** around 0.3 for balanced precision/recall\n",
    "- **Retrieve 3-5 documents** to provide sufficient context without noise\n",
    "- **Normalize embeddings** for consistent similarity calculations\n",
    "\n",
    "### Prompt Engineering\n",
    "- **Create role-specific prompts** for different user types (customers, employees, executives)\n",
    "- **Include clear instructions** for handling cases where information isn't available\n",
    "- **Use structured templates** that separate context from questions\n",
    "- **Test prompt variations** to optimize for your specific use cases\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Start with core functionality** - Get basic RAG working with your documents\n",
    "2. **Add monitoring early** - Implement logging and metrics collection\n",
    "3. **Customize for your domain** - Tailor prompts and chunking for your content\n",
    "4. **Iterate based on feedback** - Use real user interactions to improve the system\n",
    "5. **Plan for production** - Consider scalability, monitoring, and maintenance"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
